import os
import re
import torch
import gymnasium as gym
import numpy as np
import collections
from typing import Optional, List, Dict, Any

from transformers import AutoTokenizer
from peft import LoraConfig
from trl import GRPOTrainer, GRPOConfig

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ==========================================

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò ---
USE_LOCAL = False  
# True  = –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –Ω–µ –Ω—É–∂–µ–Ω).
# False = –ö–∞—á–∞—Ç—å —Å HuggingFace –∏–ª–∏ –±—Ä–∞—Ç—å –∏–∑ –∫—ç—à–∞ HF.

# –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–µ (–µ—Å–ª–∏ USE_LOCAL = True)
LOCAL_MODEL_PATH = "C:/Models/Qwen2.5-1.5B-Instruct" 

# ID –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace (–µ—Å–ª–∏ USE_LOCAL = False)
HF_MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–†–ï–î–´ –ò –ò–°–¢–û–†–ò–ò ---
ENV_ID = "CartPole-v1"
MAX_HISTORY_STEPS = 6  # –°–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —à–∞–≥–æ–≤ (Obs+Action) –ø–æ–º–Ω–∏—Ç—å. –°—Ç–∞—Ä–æ–µ –∑–∞–±—ã–≤–∞–µ—Ç—Å—è.

# --- –ù–ê–°–¢–†–û–ô–ö–ò –û–ë–£–ß–ï–ù–ò–Ø (–ø–æ–¥ RTX 4090) ---
OUTPUT_DIR = "./qwen_rl_history_output"
MAX_STEPS = 300       # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
BATCH_SIZE = 4        # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
GRAD_ACCUM = 4        # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 4 * 4 = 16)
NUM_GENERATIONS = 8   # GRPO –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 8 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ 1 –ø—Ä–æ–º–ø—Ç

# ==========================================
# 2. –ü–û–î–ì–û–¢–û–í–ö–ê –ò –£–¢–ò–õ–ò–¢–´
# ==========================================

def get_model_source():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –æ—Ç–∫—É–¥–∞ –±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å."""
    if USE_LOCAL:
        if not os.path.exists(LOCAL_MODEL_PATH):
            raise FileNotFoundError(
                f"‚ùå –û–®–ò–ë–ö–ê: –†–µ–∂–∏–º USE_LOCAL=True, –Ω–æ –ø—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {LOCAL_MODEL_PATH}"
            )
        print(f"‚úÖ [OFFLINE MODE] –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {LOCAL_MODEL_PATH}")
        return LOCAL_MODEL_PATH, {"local_files_only": True}
    else:
        print(f"üåê [ONLINE/CACHE MODE] –ò—Å–ø–æ–ª—å–∑—É–µ–º HF ID: {HF_MODEL_ID}")
        return HF_MODEL_ID, {"local_files_only": False}

def format_history_prompt(history: list) -> list:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π role/content) –≤ —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è ChatML.
    """
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: –æ–±—ä—è—Å–Ω—è–µ–º –∑–∞–¥–∞—á—É –∏ —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ XML
    messages = [
        {"role": "system", "content": (
            "You are a reinforcement learning agent controlling a CartPole system. "
            "Your goal is to balance the pole. "
            "Analyze the history of observations. "
            "Output ONLY the next action as an integer (0 or 1) inside <action> tags, like <action>1</action>."
        )}
    ]
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (Obs -> Action -> Obs ...)
    messages.extend(history)
    return messages

# ==========================================
# 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê –° –ò–°–¢–û–†–ò–ï–ô
# ==========================================

def build_dataset_with_history(tokenizer, num_samples=200):
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç, —Å–∏–º—É–ª–∏—Ä—É—è –∫–æ—Ä–æ—Ç–∫–∏–µ —ç–ø–∏–∑–æ–¥—ã –∏–≥—Ä—ã, —á—Ç–æ–±—ã –Ω–∞–ø–æ–ª–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    """
    env = gym.make(ENV_ID)
    dataset_data = []
    
    print(f"üîÑ Generating {num_samples} samples with history context...")
    
    for _ in range(num_samples):
        # –û—á–µ—Ä–µ–¥—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è "–∑–∞–±—ã–≤–∞–Ω–∏—è")
        # maxlen * 2, —Ç–∞–∫ –∫–∞–∫ —Ö—Ä–∞–Ω–∏–º –ø–∞—Ä—ã User(Obs) –∏ Assistant(Action)
        history_buffer = collections.deque(maxlen=MAX_HISTORY_STEPS * 2)
        
        obs, _ = env.reset()
        current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"
        
        # –°–ª—É—á–∞–π–Ω–∞—è –¥–ª–∏–Ω–∞ "—Ä–∞–∑–æ–≥—Ä–µ–≤–∞" –æ—Ç 0 –¥–æ 5 —à–∞–≥–æ–≤
        warmup_steps = np.random.randint(0, 6)
        
        # --- –§–ê–ó–ê –†–ê–ó–û–ì–†–ï–í–ê (–ù–∞–ø–æ–ª–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é) ---
        for _ in range(warmup_steps):
            # 1. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
            history_buffer.append({"role": "user", "content": current_obs_str})
            
            # 2. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–∏–º–∏—Ç–∞—Ü–∏—è –ø—Ä–æ—à–ª–æ–≥–æ –æ–ø—ã—Ç–∞)
            action = env.action_space.sample()
            action_str = f"<action>{action}</action>"
            history_buffer.append({"role": "assistant", "content": action_str})
            
            # 3. –®–∞–≥ —Å—Ä–µ–¥—ã
            obs, _, terminated, truncated, _ = env.step(action)
            current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"
            
            if terminated or truncated:
                obs, _ = env.reset()
                history_buffer.clear()
                current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"
        
        # --- –§–ê–ó–ê –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –ü–†–û–ú–ü–¢–ê –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø ---
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—Ç–∏—Ç—å
        history_buffer.append({"role": "user", "content": current_obs_str})
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Ç–µ–∫—Å—Ç
        messages = format_history_prompt(list(history_buffer))
        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        dataset_data.append({
            "prompt": prompt_text,
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—Å—ã—Ä–æ–µ" —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, —á—Ç–æ–±—ã reward_function –º–æ–≥–ª–∞ –µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
            "raw_state": env.unwrapped.state 
        })

    env.close()
    return dataset_data

# ==========================================
# 4. REWARD FUNCTION (–õ–û–ì–ò–ö–ê –ù–ê–ì–†–ê–î–´)
# ==========================================

def reward_function(prompts, completions, **kwargs):
    rewards = []
    env = gym.make(ENV_ID)
    
    for prompt, completion in zip(prompts, completions):
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ–π—Å—Ç–≤–∏—è
        # –ò—â–µ–º <action>X</action> –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ
        action_match = re.search(r"<action>(\d+)</action>", completion)
        if not action_match:
            action_match = re.search(r"(\d+)", completion)
            
        valid_format = False
        action = 0
        
        if action_match:
            try:
                action = int(action_match.group(1))
                if action in [0, 1]:
                    valid_format = True
            except:
                pass
        
        if not valid_format:
            rewards.append(-1.0) # –®—Ç—Ä–∞—Ñ –∑–∞ –º—É—Å–æ—Ä –Ω–∞ –≤—ã—Ö–æ–¥–µ
            continue

        # 2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã (Observation Reconstruction)
        # GRPO –ø–µ—Ä–µ–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç, –Ω–æ –Ω–µ –æ–±—ä–µ–∫—Ç —Å—Ä–µ–¥—ã.
        # –ù–∞–º –Ω—É–∂–Ω–æ –≤—ã—Ç–∞—â–∏—Ç—å –ü–û–°–õ–ï–î–ù–ï–ï –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–º–ø—Ç–∞.
        # –§–æ—Ä–º–∞—Ç –≤ —Ç–µ–∫—Å—Ç–µ: "Observation: [ 0.01  -0.02 ... ]"
        try:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π "Observation:", —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é
            last_obs_text = prompt.split("Observation:")[-1]
            obs_match = re.search(r"\[([\d\.\s\-\w]+)\]", last_obs_text)
            
            if obs_match:
                # –ü–∞—Ä—Å–∏–º —á–∏—Å–ª–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ –º–∞—Å—Å–∏–≤
                obs_values = np.fromstring(obs_match.group(1), sep=' ')
                
                # –•–ê–ö –¥–ª—è CartPole: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                env.reset()
                env.unwrapped.state = obs_values 
                
                # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                _, r, terminated, _, _ = env.step(action)
                
                # –†–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã
                current_reward = float(r)
                
                # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç XML (–ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–µ–µ –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
                if "<action>" in completion:
                    current_reward += 0.5
                
                # –°–∏–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø–∞–¥–µ–Ω–∏–µ
                if terminated:
                    current_reward = -5.0
                
                rewards.append(current_reward)
            else:
                # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –Ω–∞–π—Ç–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –≤ –ø—Ä–æ–º–ø—Ç–µ (—Å—Ç—Ä–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞)
                rewards.append(0.0)
                
        except Exception as e:
            # print(f"Env Error: {e}") # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            rewards.append(-0.5)
            
    env.close()
    return rewards

# ==========================================
# 5. –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø
# ==========================================

def main():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ray: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π —à—É–º –≤ –ª–æ–≥–∞—Ö
    os.environ["RAY_DEDUP_LOGS"] = "0"
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    model_path, model_kwargs = get_model_source()
    
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = AutoTokenizer.from_pretrained(model_path, **model_kwargs)
    tokenizer.pad_token = tokenizer.eos_token

    # 3. –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º datasets –∏–∑ HuggingFace –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å TRL
    raw_data = build_dataset_with_history(tokenizer, num_samples=500)
    from datasets import Dataset
    dataset = Dataset.from_list(raw_data)

    # 4. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DoRA (Weight-Decomposed Low-Rank Adaptation)
    # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–±—ã—á–Ω–æ–π LoRA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º
    peft_config = LoraConfig(
        r=32,               # –†–∞–Ω–≥ (Rank)
        lora_alpha=64,      # Alpha (–æ–±—ã—á–Ω–æ rank * 2)
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        use_dora=True       # –í–∫–ª—é—á–∞–µ–º DoRA
    )

    # 5. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO Trainer
    training_args = GRPOConfig(
        output_dir=OUTPUT_DIR,
        learning_rate=1e-5,          # –ê–∫–∫—É—Ä–∞—Ç–Ω—ã–π Learning Rate
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        max_steps=MAX_STEPS,
        fp16=True,                   # FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ RTX 4090
        logging_steps=10,
        save_steps=100,
        # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        num_generations=NUM_GENERATIONS, # –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        max_completion_length=32,        # –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        beta=0.04,                       # KL penalty (—á—Ç–æ–±—ã –Ω–µ —É—Ö–æ–¥–∏–ª–∞ –¥–∞–ª–µ–∫–æ –æ—Ç –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏)
        # vLLM –∏ Ray –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        use_vllm=False,                   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –¥–≤–∏–∂–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        # vllm_gpu_memory_utilization=0.3, # 30% VRAM –ø–æ–¥ Ray/vLLM, 70% –ø–æ–¥ Trainer
    )

    print(f"\nüöÄ Starting GRPO Training on {ENV_ID}...")
    print(f"   Model: {model_path}")
    print(f"   History Context: {MAX_HISTORY_STEPS} steps")
    print(f"   Device: RTX 4090 (Allocating ~30% for Inference, ~70% for Train)\n")

    # 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = GRPOTrainer(
        model=model_path,
        reward_funcs=reward_function,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
        processing_class=tokenizer,
    )

    # 7. –ó–∞–ø—É—Å–∫
    trainer.train()
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_path = os.path.join(OUTPUT_DIR, "final_model")
    trainer.save_model(final_path)
    print(f"\n‚úÖ Training finished! Model saved to: {final_path}")

if __name__ == "__main__":
    main()
