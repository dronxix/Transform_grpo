"""
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Gymnasium —Å—Ä–µ–¥ —Å Ray
–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤ 10-16x –Ω–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö
"""

import ray
import numpy as np
import pickle
import os
from typing import List, Dict
from gym_environment import GymnasiumWrapper, create_gym_environment


@ray.remote
class ParallelEnvRunner:
    """
    Ray worker –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏–∑ —Å—Ä–µ–¥—ã
    """
    
    def __init__(self, env_preset: str, seed: int = None, **env_kwargs):
        """
        Args:
            env_preset: –Ω–∞–∑–≤–∞–Ω–∏–µ preset —Å—Ä–µ–¥—ã
            seed: random seed –¥–ª—è —ç—Ç–æ–≥–æ worker
            **env_kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ä–µ–¥—ã
        """
        self.env = create_gym_environment(env_preset, seed=seed, **env_kwargs)
        self.env_preset = env_preset
        self.seed = seed
    
    def collect_episodes(self, num_episodes: int, max_episode_length: int = 1000) -> List[Dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —ç–ø–∏–∑–æ–¥—ã —Å–æ —Å–ª—É—á–∞–π–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π
        
        Args:
            num_episodes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞
            max_episode_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
            
        Returns:
            episodes: —Å–ø–∏—Å–æ–∫ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        """
        episodes = []
        
        for _ in range(num_episodes):
            obs_list = []
            action_list = []
            reward_list = []
            
            obs = self.env.reset()
            obs_list.append(obs)
            
            for step in range(max_episode_length):
                action = self.env.sample_action()
                obs, reward, done, info = self.env.step(action)
                
                action_list.append(action)
                reward_list.append(reward)
                obs_list.append(obs)
                
                if done:
                    break
            
            episodes.append({
                'observations': obs_list,
                'actions': action_list,
                'rewards': reward_list
            })
        
        return episodes
    
    def collect_episodes_with_policy(
        self, 
        num_episodes: int,
        policy_fn,
        max_episode_length: int = 1000
    ) -> List[Dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —ç–ø–∏–∑–æ–¥—ã —Å –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π (–¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π)
        
        Args:
            num_episodes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤
            policy_fn: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ (obs) -> action
            max_episode_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
            
        Returns:
            episodes: —Å–ø–∏—Å–æ–∫ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        """
        episodes = []
        
        for _ in range(num_episodes):
            obs_list = []
            action_list = []
            reward_list = []
            
            obs = self.env.reset()
            obs_list.append(obs)
            
            for step in range(max_episode_length):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–∞–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É
                action = policy_fn(obs)
                obs, reward, done, info = self.env.step(action)
                
                action_list.append(action)
                reward_list.append(reward)
                obs_list.append(obs)
                
                if done:
                    break
            
            episodes.append({
                'observations': obs_list,
                'actions': action_list,
                'rewards': reward_list
            })
        
        return episodes
    
    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ä–µ–¥–µ"""
        return {
            'env_preset': self.env_preset,
            'obs_dim': self.env.obs_dim,
            'action_dim': self.env.action_dim,
            'seed': self.seed
        }


def parallel_collect_data(
    env_preset: str = 'cartpole',
    num_episodes: int = 1000,
    max_episode_length: int = 500,
    num_workers: int = None,
    save_path: str = 'data/gym_trajectories.pkl',
    seed: int = 42,
    verbose: bool = True,
    **env_kwargs
):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å Ray
    
    Args:
        env_preset: –Ω–∞–∑–≤–∞–Ω–∏–µ preset —Å—Ä–µ–¥—ã
        num_episodes: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤
        max_episode_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
        num_workers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö workers (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU)
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        seed: –±–∞–∑–æ–≤—ã–π random seed
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        **env_kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ä–µ–¥—ã
        
    Returns:
        trajectories: —Å–ø–∏—Å–æ–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
        env_config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ä–µ–¥—ã
    """
    import time
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Ray –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω
    if not ray.is_initialized():
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU
        import multiprocessing
        num_cpus = multiprocessing.cpu_count()
        
        if num_workers is None:
            num_workers = min(num_cpus, 16)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 16 workers
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø RAY")
            print(f"{'='*80}")
            print(f"–î–æ—Å—Ç—É–ø–Ω–æ CPU: {num_cpus}")
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º workers: {num_workers}")
        
        ray.init(num_cpus=num_workers, ignore_reinit_error=True)
    else:
        if num_workers is None:
            num_workers = min(ray.cluster_resources().get('CPU', 1), 16)
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–•")
        print(f"{'='*80}")
        print(f"–°—Ä–µ–¥–∞: {env_preset}")
        print(f"–≠–ø–∏–∑–æ–¥–æ–≤: {num_episodes}")
        print(f"Workers: {num_workers}")
        print(f"–≠–ø–∏–∑–æ–¥–æ–≤ –Ω–∞ worker: ~{num_episodes // num_workers}")
    
    start_time = time.time()
    
    # –°–æ–∑–¥–∞–µ–º workers —Å —Ä–∞–∑–Ω—ã–º–∏ seeds
    workers = []
    for i in range(num_workers):
        worker_seed = seed + i if seed is not None else None
        worker = ParallelEnvRunner.remote(
            env_preset=env_preset,
            seed=worker_seed,
            **env_kwargs
        )
        workers.append(worker)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ä–µ–¥—ã –æ—Ç –ø–µ—Ä–≤–æ–≥–æ worker
    env_config = ray.get(workers[0].get_stats.remote())
    env_config['env_preset'] = env_preset
    env_config['seed'] = seed
    
    if verbose:
        print(f"\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ä–µ–¥—ã:")
        print(f"  Observation dim: {env_config['obs_dim']}")
        print(f"  Action dim: {env_config['action_dim']}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–ø–∏–∑–æ–¥—ã –º–µ–∂–¥—É workers
    episodes_per_worker = num_episodes // num_workers
    remaining_episodes = num_episodes % num_workers
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä
    if verbose:
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä...")
    
    futures = []
    for i, worker in enumerate(workers):
        # –ü–æ—Å–ª–µ–¥–Ω–µ–º—É worker –¥–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–ø–∏–∑–æ–¥—ã
        worker_episodes = episodes_per_worker + (remaining_episodes if i == len(workers) - 1 else 0)
        
        future = worker.collect_episodes.remote(
            num_episodes=worker_episodes,
            max_episode_length=max_episode_length
        )
        futures.append(future)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if verbose:
        print(f"–û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è workers...")
    
    all_episodes = []
    completed_workers = 0
    
    # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ray.wait –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    while futures:
        done_futures, futures = ray.wait(futures, num_returns=1)
        episodes = ray.get(done_futures[0])
        all_episodes.extend(episodes)
        completed_workers += 1
        
        if verbose:
            print(f"  Worker {completed_workers}/{num_workers} –∑–∞–≤–µ—Ä—à–µ–Ω "
                  f"({len(all_episodes)}/{num_episodes} —ç–ø–∏–∑–æ–¥–æ–≤)")
    
    collection_time = time.time() - start_time
    
    if verbose:
        print(f"\n‚úì –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {collection_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_steps = sum(len(ep['actions']) for ep in all_episodes)
        total_reward = sum(sum(ep['rewards']) for ep in all_episodes)
        avg_length = total_steps / len(all_episodes)
        avg_reward = total_reward / len(all_episodes)
        
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  –°–æ–±—Ä–∞–Ω–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(all_episodes)}")
        print(f"  –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.2f}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.2f}")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {len(all_episodes) / collection_time:.2f} —ç–ø–∏–∑–æ–¥–æ–≤/—Å–µ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        data = {
            'trajectories': all_episodes,
            'env_config': env_config,
            'collection_time': collection_time,
            'num_workers': num_workers
        }
        
        with open(save_path, 'wb') as f:
            pickle.dump(data, f)
        
        if verbose:
            print(f"\n‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    
    return all_episodes, env_config


def parallel_collect_multiple_envs(
    env_presets: List[str],
    num_episodes_per_env: int = 500,
    max_episode_length: int = 500,
    num_workers: int = None,
    save_dir: str = 'data/multi_env',
    seed: int = 42,
    verbose: bool = True
):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ä–µ–¥ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    
    Args:
        env_presets: —Å–ø–∏—Å–æ–∫ preset —Å—Ä–µ–¥
        num_episodes_per_env: —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ä–µ–¥—ã
        max_episode_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞
        num_workers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –º–µ–∂–¥—É —Å—Ä–µ–¥–∞–º–∏)
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        seed: –±–∞–∑–æ–≤—ã–π seed
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        
    Returns:
        all_trajectories: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        env_configs: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö —Å—Ä–µ–¥
    """
    if not ray.is_initialized():
        import multiprocessing
        total_cpus = multiprocessing.cpu_count()
        if num_workers is None:
            num_workers = min(total_cpus, 16)
        ray.init(num_cpus=num_workers, ignore_reinit_error=True)
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† –ò–ó –ù–ï–°–ö–û–õ–¨–ö–ò–• –°–†–ï–î")
        print(f"{'='*80}")
        print(f"–°—Ä–µ–¥—ã: {env_presets}")
        print(f"–≠–ø–∏–∑–æ–¥–æ–≤ –Ω–∞ —Å—Ä–µ–¥—É: {num_episodes_per_env}")
        print(f"Workers: {num_workers}")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–ª—è –≤—Å–µ—Ö —Å—Ä–µ–¥ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    futures = []
    for i, preset in enumerate(env_presets):
        save_path = os.path.join(save_dir, f'{preset}_trajectories.pkl')
        
        # –ö–∞–∂–¥–∞—è —Å—Ä–µ–¥–∞ –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ—é –¥–æ–ª—é workers
        workers_per_env = max(1, num_workers // len(env_presets))
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        future = ray.remote(parallel_collect_data).remote(
            env_preset=preset,
            num_episodes=num_episodes_per_env,
            max_episode_length=max_episode_length,
            num_workers=workers_per_env,
            save_path=save_path,
            seed=seed + i * 1000,
            verbose=False  # –û—Ç–∫–ª—é—á–∞–µ–º verbose –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ä–µ–¥—ã
        )
        futures.append((preset, future))
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_trajectories = []
    env_configs = []
    
    for preset, future in futures:
        if verbose:
            print(f"\n–û–∂–∏–¥–∞–µ–º {preset}...")
        
        trajectories, env_config = ray.get(future)
        all_trajectories.extend(trajectories)
        env_configs.append(env_config)
        
        if verbose:
            print(f"‚úì {preset}: {len(trajectories)} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    combined_path = os.path.join(save_dir, 'combined_trajectories.pkl')
    data = {
        'trajectories': all_trajectories,
        'env_configs': env_configs
    }
    
    with open(combined_path, 'wb') as f:
        pickle.dump(data, f)
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"‚úì –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {combined_path}")
        print(f"  –í—Å–µ–≥–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π: {len(all_trajectories)}")
        print(f"  –ò–∑ {len(env_configs)} —Å—Ä–µ–¥")
        print(f"{'='*80}")
    
    return all_trajectories, env_configs


def benchmark_parallel_vs_sequential():
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞
    """
    import time
    from gym_environment import create_gym_environment, GymnasiumTrajectoryCollector
    
    num_episodes = 100
    
    print(f"\n{'='*80}")
    print(f"–ë–ï–ù–ß–ú–ê–†–ö: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô vs –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–ô –°–ë–û–†")
    print(f"{'='*80}")
    print(f"–≠–ø–∏–∑–æ–¥–æ–≤: {num_episodes}")
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä
    print(f"\n1. –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–ô –°–ë–û–†:")
    start = time.time()
    env = create_gym_environment('cartpole', seed=42)
    collector = GymnasiumTrajectoryCollector(env, max_episode_length=500)
    trajectories_seq = collector.collect_random_trajectories(num_episodes, verbose=False)
    env.close()
    time_seq = time.time() - start
    print(f"   –í—Ä–µ–º—è: {time_seq:.2f} —Å–µ–∫—É–Ω–¥")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä
    print(f"\n2. –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† (Ray):")
    start = time.time()
    trajectories_par, _ = parallel_collect_data(
        env_preset='cartpole',
        num_episodes=num_episodes,
        max_episode_length=500,
        seed=42,
        save_path=None,
        verbose=False
    )
    time_par = time.time() - start
    print(f"   –í—Ä–µ–º—è: {time_par:.2f} —Å–µ–∫—É–Ω–¥")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    speedup = time_seq / time_par
    print(f"\n{'='*80}")
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"  –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: {time_seq:.2f}s")
    print(f"  –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: {time_par:.2f}s")
    print(f"  –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.2f}x")
    print(f"{'='*80}")
    
    # –û—á–∏—â–∞–µ–º Ray
    ray.shutdown()


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä
    print("\n" + "="*80)
    print("–ü–†–ò–ú–ï–† 1: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–•")
    print("="*80)
    
    trajectories, env_config = parallel_collect_data(
        env_preset='cartpole',
        num_episodes=100,
        max_episode_length=200,
        num_workers=4,
        save_path='data/parallel_cartpole.pkl',
        seed=42
    )
    
    # –ü—Ä–∏–º–µ—Ä 2: –ë–µ–Ω—á–º–∞—Ä–∫
    print("\n" + "="*80)
    print("–ü–†–ò–ú–ï–† 2: –ë–ï–ù–ß–ú–ê–†–ö")
    print("="*80)
    
    benchmark_parallel_vs_sequential()
    
    # –ü—Ä–∏–º–µ—Ä 3: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ä–µ–¥ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    print("\n" + "="*80)
    print("–ü–†–ò–ú–ï–† 3: –ú–£–õ–¨–¢–ò-–°–†–ï–î–ê")
    print("="*80)
    
    all_traj, all_configs = parallel_collect_multiple_envs(
        env_presets=['cartpole', 'mountain_car'],
        num_episodes_per_env=50,
        num_workers=8,
        save_dir='data/multi_env_parallel'
    )
    
    ray.shutdown()
    print("\n‚úì –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
