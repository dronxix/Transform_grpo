"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Trainer —Å:
- Mixed Precision Training (AMP) - 2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ
- Gradient Accumulation - –±–æ–ª—å—à–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –±–∞—Ç—á–∏
- torch.compile - 1.5-2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ (PyTorch 2.0+)
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataLoader
- Multi-GPU support (DDP)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import GradScaler
import os
import sys
import json
import time
from datetime import datetime
from tqdm import tqdm

from model import DecisionTransformer
from gym_data_preparation import load_gym_trajectories, create_dataloaders

if torch.cuda.is_available():
    # –ù–æ–≤—ã–π API –¥–ª—è TF32 (PyTorch 2.9+)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

class OptimizedTrainer:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å Mixed Precision, Gradient Accumulation, torch.compile
    """
    
    def __init__(
        self,
        model,
        train_loader,
        val_loader,
        config,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Windows - torch.compile –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        is_windows = sys.platform.startswith('win')
        if is_windows and config.get('use_compile', False):
            print("‚ö†Ô∏è  Windows –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: torch.compile –æ—Ç–∫–ª—é—á–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç Triton, –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ Windows)")
            config['use_compile'] = False
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º torch.compile –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (PyTorch 2.0+)
        if config.get('use_compile', False) and hasattr(torch, 'compile'):
            print("üöÄ –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å torch.compile...")
            model = torch.compile(model, mode='reduce-overhead')
            print("‚úì –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞!")
        
        self.model = model.to(device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            betas=(0.9, 0.999),
            fused=True if device == 'cuda' else False  # Fused optimizer –¥–ª—è GPU
        )
        
        # Learning rate scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config['num_epochs'],
            eta_min=config['learning_rate'] * 0.1
        )
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)
        
        # Mixed Precision Training
        self.use_amp = config.get('use_amp', False) and device == 'cuda'
        if self.use_amp:
            self.scaler = GradScaler()
            print("‚úì Mixed Precision Training –≤–∫–ª—é—á–µ–Ω")
        
        # Gradient Accumulation
        self.accumulation_steps = config.get('accumulation_steps', 1)
        if self.accumulation_steps > 1:
            print(f"‚úì Gradient Accumulation: {self.accumulation_steps} —à–∞–≥–æ–≤")
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {config['batch_size'] * self.accumulation_steps}")
        
        # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.writer = SummaryWriter(log_dir=config['log_dir'])
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.best_val_loss = float('inf')
        self.global_step = 0
        self.epoch = 0
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(config['checkpoint_dir'], exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
        with open(os.path.join(config['checkpoint_dir'], 'config.json'), 'w') as f:
            json.dump(config, f, indent=4)
        
        print(f"\n{'='*80}")
        print(f"Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"  Device: {self.device}")
        print(f"  Mixed Precision: {self.use_amp}")
        print(f"  Gradient Accumulation: {self.accumulation_steps}")
        print(f"  Batch size: {config['batch_size']}")
        print(f"  Effective batch size: {config['batch_size'] * self.accumulation_steps}")
        print(f"{'='*80}\n")
    
    def train_epoch(self):
        """–û–¥–∏–Ω epoch –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch + 1}/{self.config['num_epochs']}")
        
        # –î–ª—è gradient accumulation
        self.optimizer.zero_grad()
        
        for batch_idx, batch in enumerate(pbar):
            observations = batch['observations'].to(self.device, non_blocking=True)
            actions = batch['actions'].to(self.device, non_blocking=True)
            returns_to_go = batch['returns_to_go'].to(self.device, non_blocking=True)  # –ù–û–í–û–ï!
            targets = batch['targets'].to(self.device, non_blocking=True)
            mask = batch['mask'].to(self.device, non_blocking=True)
            
            # Mixed Precision Training
            if self.use_amp:
                with torch.amp.autocast(device_type='cuda'):
                    # –ò–ó–ú–ï–ù–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º returns_to_go
                    logits, lb_loss = self.model(observations, actions, returns_to_go)
                    
                    # Loss
                    batch_size, seq_len, action_dim = logits.shape
                    logits_flat = logits.view(-1, action_dim)
                    targets_flat = targets.view(-1)
                    action_loss = self.criterion(logits_flat, targets_flat)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º load balancing loss
                    loss = action_loss + self.config.get('load_balancing_loss_coef', 0.01) * lb_loss
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è gradient accumulation
                    loss = loss / self.accumulation_steps
                
                self.scaler.scale(loss).backward()
                
            else:
                # –ò–ó–ú–ï–ù–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º returns_to_go
                logits, lb_loss = self.model(observations, actions, returns_to_go)
                
                # Loss
                batch_size, seq_len, action_dim = logits.shape
                logits_flat = logits.view(-1, action_dim)
                targets_flat = targets.view(-1)
                action_loss = self.criterion(logits_flat, targets_flat)
                
                # –î–æ–±–∞–≤–ª—è–µ–º load balancing loss
                loss = action_loss + self.config.get('load_balancing_loss_coef', 0.01) * lb_loss
                
                loss = loss / self.accumulation_steps
                loss.backward()
            
            # Optimizer step –∫–∞–∂–¥—ã–µ accumulation_steps –±–∞—Ç—á–µ–π
            if (batch_idx + 1) % self.accumulation_steps == 0:
                if self.use_amp:
                    # Gradient clipping
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['grad_clip']
                    )
                    
                    # Optimizer step
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['grad_clip']
                    )
                    
                    # Optimizer step
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # –ú–µ—Ç—Ä–∏–∫–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥)
            with torch.no_grad():
                predictions = logits_flat.argmax(dim=-1)
                valid_mask = (targets_flat != -100)
                correct = ((predictions == targets_flat) & valid_mask).sum().item()
                total_correct += correct
                total_samples += valid_mask.sum().item()
            
            # –†–µ–∞–ª—å–Ω—ã–π loss (—É–º–Ω–æ–∂–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ)
            actual_loss = loss.item() * self.accumulation_steps
            total_loss += actual_loss
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if self.global_step % self.config['log_interval'] == 0:
                valid_samples = valid_mask.sum().item()
                batch_accuracy = correct / valid_samples if valid_samples > 0 else 0.0
                self.writer.add_scalar('train/loss', actual_loss, self.global_step)
                self.writer.add_scalar('train/accuracy', batch_accuracy, self.global_step)
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            self.global_step += 1
            
            # Update progress bar
            valid_samples_batch = valid_mask.sum().item()
            batch_acc = correct / valid_samples_batch if valid_samples_batch > 0 else 0.0
            pbar.set_postfix({
                'loss': f'{actual_loss:.4f}',
                'acc': f'{batch_acc:.4f}'
            })
        
        avg_loss = total_loss / len(self.train_loader)
        avg_accuracy = total_correct / total_samples if total_samples > 0 else 0.0
        
        return avg_loss, avg_accuracy
    
    @torch.no_grad()
    def validate(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        self.model.eval()
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        for batch in tqdm(self.val_loader, desc="Validation"):
            observations = batch['observations'].to(self.device, non_blocking=True)
            actions = batch['actions'].to(self.device, non_blocking=True)
            returns_to_go = batch['returns_to_go'].to(self.device, non_blocking=True)  # –ù–û–í–û–ï!
            targets = batch['targets'].to(self.device, non_blocking=True)
            
            if self.use_amp:
                with torch.amp.autocast(device_type='cuda'):
                    logits, lb_loss = self.model(observations, actions, returns_to_go)  # –ò–ó–ú–ï–ù–ï–ù–û
            else:
                logits, lb_loss = self.model(observations, actions, returns_to_go)  # –ò–ó–ú–ï–ù–ï–ù–û
            
            # Loss
            batch_size, seq_len, action_dim = logits.shape
            logits_flat = logits.view(-1, action_dim)
            targets_flat = targets.view(-1)
            
            action_loss = self.criterion(logits_flat, targets_flat)
            loss = action_loss + self.config.get('load_balancing_loss_coef', 0.01) * lb_loss
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            predictions = logits_flat.argmax(dim=-1)
            valid_mask = (targets_flat != -100)
            correct = ((predictions == targets_flat) & valid_mask).sum().item()
            
            total_loss += loss.item()
            total_correct += correct
            total_samples += valid_mask.sum().item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_accuracy = total_correct / total_samples if total_samples > 0 else 0.0
        
        return avg_loss, avg_accuracy
    
    def save_checkpoint(self, filename='checkpoint.pt', is_best=False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞, –±–µ—Ä–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
        model_to_save = self.model._orig_mod if hasattr(self.model, '_orig_mod') else self.model
        
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': model_to_save.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.use_amp:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        path = os.path.join(self.config['checkpoint_dir'], filename)
        torch.save(checkpoint, path)
        print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
        
        if is_best:
            best_path = os.path.join(self.config['checkpoint_dir'], 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_path}")
    
    def load_checkpoint(self, filename='checkpoint.pt'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        path = os.path.join(self.config['checkpoint_dir'], filename)
        
        if not os.path.exists(path):
            print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
            return False
        
        checkpoint = torch.load(path, map_location=self.device)
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª
        model_to_load = self.model._orig_mod if hasattr(self.model, '_orig_mod') else self.model
        model_to_load.load_state_dict(checkpoint['model_state_dict'])
        
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        if self.use_amp and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {path}")
        print(f"Resuming from epoch {self.epoch + 1}")
        
        return True
    
    def train(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        print("=" * 80)
        print("–ù–ê–ß–ò–ù–ê–ï–ú –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï")
        print("=" * 80)
        print(f"Device: {self.device}")
        print(f"Total epochs: {self.config['num_epochs']}")
        print(f"Train batches: {len(self.train_loader)}")
        print(f"Val batches: {len(self.val_loader)}")
        print(f"Mixed Precision: {self.use_amp}")
        print(f"Gradient Accumulation: {self.accumulation_steps}")
        print("=" * 80)
        
        start_time = time.time()
        
        for epoch in range(self.epoch, self.config['num_epochs']):
            self.epoch = epoch
            epoch_start = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate()
            
            # Scheduler step
            self.scheduler.step()
            
            epoch_time = time.time() - epoch_start
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.writer.add_scalar('epoch/train_loss', train_loss, epoch)
            self.writer.add_scalar('epoch/train_accuracy', train_acc, epoch)
            self.writer.add_scalar('epoch/val_loss', val_loss, epoch)
            self.writer.add_scalar('epoch/val_accuracy', val_acc, epoch)
            self.writer.add_scalar('epoch/time', epoch_time, epoch)
            
            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            print(f"\nEpoch {epoch + 1}/{self.config['num_epochs']}")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
            print(f"Time: {epoch_time:.2f}s | LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
            if (epoch + 1) % self.config['save_interval'] == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch + 1}.pt')
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('checkpoint.pt', is_best=True)
                print(f"‚úì –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! Val Loss: {val_loss:.4f}")
            
            print("-" * 80)
        
        total_time = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"{'='*80}")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time / 3600:.2f} —á–∞—Å–æ–≤ ({total_time:.2f} —Å–µ–∫—É–Ω–¥)")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É: {total_time / self.config['num_epochs']:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–õ—É—á—à–∏–π val loss: {self.best_val_loss:.4f}")
        print(f"{'='*80}")
        
        self.writer.close()
