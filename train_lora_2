import os
import re
import torch
import gymnasium as gym
import numpy as np
import collections
import shutil
from typing import List, Dict

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from trl import GRPOTrainer, GRPOConfig

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ==========================================

USE_LOCAL = False
LOCAL_MODEL_PATH = "C:/Models/Qwen2.5-1.5B-Instruct"
HF_MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"

ENV_ID = "CartPole-v1"
MAX_HISTORY_STEPS = 6

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Online —Ü–∏–∫–ª–∞
ITERATIONS = 50           # –°–∫–æ–ª—å–∫–æ —Ü–∏–∫–ª–æ–≤ "–°–±–æ—Ä -> –û–±—É—á–µ–Ω–∏–µ" —Å–¥–µ–ª–∞—Ç—å
SAMPLES_PER_ITER = 64     # –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–æ–±–∏—Ä–∞—Ç—å –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ (—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Å–±–æ—Ä–∞)
TRAIN_EPOCHS_PER_ITER = 1 # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ–≥–Ω–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ GRPO
OUTPUT_DIR = "./qwen_online_rl"
BATCH_SIZE = 4
GRAD_ACCUM = 4
NUM_GENERATIONS = 8

# ==========================================
# 2. –£–¢–ò–õ–ò–¢–´
# ==========================================

def get_model_path():
    if USE_LOCAL and os.path.exists(LOCAL_MODEL_PATH):
        return LOCAL_MODEL_PATH, {"local_files_only": True}
    return HF_MODEL_ID, {"local_files_only": False}

def format_history_prompt(history: list) -> list:
    messages = [
        {"role": "system", "content": (
            "You are a reinforcement learning agent controlling a CartPole system. "
            "Output ONLY the next action as an integer (0 or 1) inside <action> tags."
        )}
    ]
    messages.extend(history)
    return messages

# ==========================================
# 3. –°–ë–û–† –î–ê–ù–ù–´–• –í –†–ï–ê–õ–¨–ù–û–ú –í–†–ï–ú–ï–ù–ò
# ==========================================

def collect_live_data(model, tokenizer, num_samples=32, device="cuda"):
    """
    –ê–≥–µ–Ω—Ç –∏–≥—Ä–∞–µ—Ç –≤ –∏–≥—Ä—É —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ (Prompts).
    """
    env = gym.make(ENV_ID)
    collected_data = []
    
    print(f"üéÆ [Rollout] Collecting {num_samples} steps using current policy...")
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    model.eval()
    
    samples_collected = 0
    
    # –ë—É—Ñ–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞
    history_buffer = collections.deque(maxlen=MAX_HISTORY_STEPS * 2)
    obs, _ = env.reset()
    current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"
    
    while samples_collected < num_samples:
        # 1. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        history_buffer.append({"role": "user", "content": current_obs_str})
        messages = format_history_prompt(list(history_buffer))
        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç (–ü—Ä–æ–º–ø—Ç + –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        # –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–Ω–æ "–≤–æ–ø—Ä–æ—Å" (—Å–∏—Ç—É–∞—Ü–∏—é), –æ—Ç–≤–µ—Ç –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π GRPO –±—É–¥–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        collected_data.append({
            "prompt": prompt_text,
            "raw_state": env.unwrapped.state 
        })
        samples_collected += 1

        # 3. –ú–æ–¥–µ–ª—å –¥–µ–ª–∞–µ—Ç —Ö–æ–¥ (Inference)
        inputs = tokenizer(prompt_text, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=16, 
                do_sample=True, # –ù–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ä–µ–¥—ã
                temperature=0.8
            )
        
        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        
        # –ü–∞—Ä—Å–∏–º –¥–µ–π—Å—Ç–≤–∏–µ
        action_match = re.search(r"<action>(\d+)</action>", response)
        if not action_match: action_match = re.search(r"(\d+)", response)
        
        action = 0
        if action_match:
            try:
                val = int(action_match.group(1))
                if val in [0, 1]: action = val
            except: pass
            
        # 4. –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
        history_buffer.append({"role": "assistant", "content": f"<action>{action}</action>"})
        obs, reward, terminated, truncated, _ = env.step(action)
        current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"
        
        # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∫–æ–Ω—á–∏–ª—Å—è
        if terminated or truncated:
            obs, _ = env.reset()
            history_buffer.clear()
            current_obs_str = f"Observation: {np.array2string(obs, precision=3)}"

    env.close()
    model.train() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    return collected_data

# ==========================================
# 4. –§–£–ù–ö–¶–ò–Ø –ù–ê–ì–†–ê–î–´ (–¢–∞ –∂–µ —Å–∞–º–∞—è)
# ==========================================

def reward_function(prompts, completions, **kwargs):
    rewards = []
    env = gym.make(ENV_ID)
    
    for prompt, completion in zip(prompts, completions):
        action_match = re.search(r"<action>(\d+)</action>", completion)
        if not action_match: action_match = re.search(r"(\d+)", completion)
        
        action = 0
        valid = False
        if action_match:
            try:
                action = int(action_match.group(1))
                if action in [0, 1]: valid = True
            except: pass
        
        if not valid:
            rewards.append(-1.0)
            continue

        try:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            last_obs_text = prompt.split("Observation:")[-1]
            obs_match = re.search(r"\[([\d\.\s\-\w]+)\]", last_obs_text)
            
            if obs_match:
                obs_values = np.fromstring(obs_match.group(1), sep=' ')
                env.reset()
                env.unwrapped.state = obs_values 
                
                _, r, terminated, _, _ = env.step(action)
                
                curr_reward = float(r)
                if "<action>" in completion: curr_reward += 0.5
                if terminated: curr_reward = -5.0
                rewards.append(curr_reward)
            else:
                rewards.append(0.0)
        except:
            rewards.append(-0.5)
            
    env.close()
    return rewards

# ==========================================
# 5. ONLINE –¶–ò–ö–õ
# ==========================================

def main():
    torch.cuda.empty_cache()
    model_path, model_kwargs = get_model_path()
    
    print("‚è≥ Loading Base Model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, **model_kwargs)
    tokenizer.pad_token = tokenizer.eos_token
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å 1 —Ä–∞–∑
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        torch_dtype=torch.float16,
        device_map="cuda:0",
        **model_kwargs
    )
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º LoRA/DoRA —Å—Ä–∞–∑—É
    peft_config = LoraConfig(
        r=32, lora_alpha=64,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", use_dora=True
    )
    model = get_peft_model(model, peft_config)
    print("‚úÖ Model loaded with DoRA adapters.")

    # --- –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø ---
    for iteration in range(1, ITERATIONS + 1):
        print(f"\n{'='*40}")
        print(f"üîÑ ITERATION {iteration}/{ITERATIONS}")
        print(f"{'='*40}")
        
        # 1. –°–ë–û–† –î–ê–ù–ù–´–• (–û–ø—ã—Ç)
        # –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ–±–µ –ø—Ä–æ–º–ø—Ç—ã, –∏–≥—Ä–∞—è –≤ —Å—Ä–µ–¥—É
        raw_data = collect_live_data(
            model, tokenizer, 
            num_samples=SAMPLES_PER_ITER, 
            device="cuda"
        )
        
        from datasets import Dataset
        dataset = Dataset.from_list(raw_data)
        
        # 2. –û–ë–£–ß–ï–ù–ò–ï (GRPO Update)
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Trainer –Ω–∞ –∫–æ—Ä–æ—Ç–∫—É—é –ø—Ä–æ–±–µ–∂–∫—É
        training_args = GRPOConfig(
            output_dir=f"{OUTPUT_DIR}/iter_{iteration}",
            learning_rate=1e-5,
            per_device_train_batch_size=BATCH_SIZE,
            gradient_accumulation_steps=GRAD_ACCUM,
            max_steps=10, # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –Ω–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            fp16=True,
            logging_steps=1,
            num_generations=NUM_GENERATIONS,
            max_completion_length=32,
            beta=0.04,
            use_vllm=False, # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            report_to="none" # –ß—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –≤ wandb
        )
        
        # –í–∞–∂–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º model object, –∞ –Ω–µ –ø—É—Ç—å, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ—Å–∞
        trainer = GRPOTrainer(
            model=model, # <-- –¢–ï–ö–£–©–ê–Ø –ú–û–î–ï–õ–¨
            reward_funcs=reward_function,
            args=training_args,
            train_dataset=dataset,
            processing_class=tokenizer,
        )
        
        print("üèãÔ∏è Training on collected experience...")
        trainer.train()
        
        # 3. –û–ß–ò–°–¢–ö–ê (–ß—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏–≤–∞—Ç—å –ø–∞–º—è—Ç—å)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–µ—Ä–∂–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
        # TRL –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ in-place, —Ç–∞–∫ —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `model` —Ç–µ–ø–µ—Ä—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞.
        
        # –û—á–∏—â–∞–µ–º trainer, –Ω–æ –ù–ï –º–æ–¥–µ–ª—å
        del trainer
        torch.cuda.empty_cache()
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
        if iteration % 5 == 0:
            save_path = os.path.join(OUTPUT_DIR, f"checkpoint_{iteration}")
            model.save_pretrained(save_path)
            print(f"üíæ Checkpoint saved: {save_path}")

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_path = os.path.join(OUTPUT_DIR, "final_online_model")
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)
    print(f"\nüèÅ Online Training Finished! Saved to {final_path}")

if __name__ == "__main__":
    main()