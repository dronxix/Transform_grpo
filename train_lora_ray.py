import os
import ray
import gymnasium as gym
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, PeftModel
import numpy as np
import re
import shutil

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
OUTPUT_DIR = "./qwen_rl_checkpoints" # –ü–∞–ø–∫–∞ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤

# –û–ø—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
USE_DORA = True       # True = DoRA (–ª—É—á—à–µ), False = –æ–±—ã—á–Ω–∞—è LoRA
FULL_FINETUNE = False # –ï—Å–ª–∏ True, –æ—Ç–∫–ª—é—á–∞–µ—Ç LoRA/DoRA –∏ —É—á–∏—Ç –≤—Å–µ (–Ω—É–∂–Ω–æ –º–Ω–æ–≥–æ VRAM!)

MAX_HISTORY = 5
GROUP_SIZE = 8        # –ë–æ–ª—å—à–µ –±–∞—Ç—á = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç
MAX_STEPS = 500       # CartPole-v1 –º–∞–∫—Å 500
LEARNING_RATE = 5e-6  # –î–ª—è DoRA/FullFT –ª—É—á—à–µ –ø–æ–º–µ–Ω—å—à–µ
NUM_ITERATIONS = 50
SAVE_EVERY = 5        # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π

# --- 1. Ray Worker ---
@ray.remote
class EnvWorker:
    def __init__(self):
        self.env = gym.make("CartPole-v1")
        
    def reset(self):
        self.state, _ = self.env.reset()
        self.history = [] 
        self.done = False
        self.total_reward = 0
        return self._get_obs_text()

    def step(self, action):
        if self.done:
            return self._get_obs_text(), self.total_reward, True, {}
            
        next_state, reward, terminated, truncated, _ = self.env.step(action)
        self.state = next_state
        self.total_reward += reward
        
        if terminated or truncated:
            self.done = True
            
        return self._get_obs_text(), self.total_reward, self.done, {}
    
    def _get_obs_text(self):
        obs = self.state
        text = f"Pos:{obs[0]:.2f}, Vel:{obs[1]:.2f}, Angle:{obs[2]:.2f}, AngVel:{obs[3]:.2f}"
        self.history.append(text)
        if len(self.history) > MAX_HISTORY:
            self.history.pop(0)
        return self.history, self.total_reward

# --- 2. –£—Ç–∏–ª–∏—Ç—ã ---
def parse_action(text):
    match = re.search(r"<action>\s*(\d+)\s*</action>", text)
    if match:
        try:
            a = int(match.group(1))
            if a in [0, 1]: return a
        except: pass
    if "1" in text[-10:]: return 1
    if "0" in text[-10:]: return 0
    return np.random.choice([0, 1])

def format_batch_prompts(batch_histories, tokenizer):
    SYSTEM_PROMPT = "You are a RL agent. Balance the pole. Output <thought>...</thought> and <action>0 or 1</action>."
    batch_texts = []
    for history in batch_histories:
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        context_str = "\n".join([f"S{i}:{h}" for i, h in enumerate(history)])
        user_content = f"History:\n{context_str}\nAction?"
        messages.append({"role": "user", "content": user_content})
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        batch_texts.append(text)
    return batch_texts

# --- 3. –§—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---
def save_checkpoint(model, tokenizer, iteration, reward, is_best=False):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä—ã (–∏–ª–∏ –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å) –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    
    # –ò–º—è –ø–∞–ø–∫–∏
    if is_best:
        save_path = os.path.join(OUTPUT_DIR, "best_model")
        print(f"üî• New Best Reward ({reward:.1f})! Saving to {save_path}...")
    else:
        save_path = os.path.join(OUTPUT_DIR, f"checkpoint_{iteration}")
        print(f"üíæ Saving checkpoint to {save_path}...")
        
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
    os.makedirs(save_path, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (PEFT —Å–æ—Ö—Ä–∞–Ω–∏—Ç —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä—ã, FullFT - –≤—Å—ë)
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∫–∞–∫–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –±—ã–ª–∞)
    with open(os.path.join(save_path, "metrics.txt"), "w") as f:
        f.write(f"iteration: {iteration}\nreward: {reward}\n")

# --- 4. Main Setup ---
# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏ ray
ray.init(ignore_reinit_error=True, log_to_driver=False)

print(f"–ó–∞–≥—Ä—É–∑–∫–∞ Qwen (DoRA={USE_DORA}, FullFT={FULL_FINETUNE})...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.eos_token

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, 
    device_map="auto",
    dtype=torch.float16,
    # use_cache=False –Ω—É–∂–Ω–æ –¥–ª—è Gradient Checkpointing (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏), –µ—Å–ª–∏ FullFT
    use_cache=not FULL_FINETUNE 
)

if not FULL_FINETUNE:
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ DoRA / LoRA
    peft_config = LoraConfig(
        r=16, 
        lora_alpha=32, 
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], # –£—á–∏–º –≤—Å–µ –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏
        lora_dropout=0.05, 
        bias="none", 
        task_type="CAUSAL_LM",
        use_dora=USE_DORA # <-- –í–æ—Ç —Ç—É—Ç –≤–∫–ª—é—á–∞–µ—Ç—Å—è –º–∞–≥–∏—è DoRA
    )
    model = get_peft_model(model, peft_config)
    print("PEFT (LoRA/DoRA) mode active.")
    model.print_trainable_parameters()
else:
    print("Full Fine-Tuning mode active. Warning: High VRAM usage.")

optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
workers = [EnvWorker.remote() for _ in range(GROUP_SIZE)]

# --- 5. Training Loop ---

best_avg_reward = -float('inf')

print(f"\nüöÄ Start Training. Output dir: {OUTPUT_DIR}")

try:
    for it in range(1, NUM_ITERATIONS + 1):
        print(f"\n--- Iteration {it}/{NUM_ITERATIONS} ---")
        
        # --- A. Rollout (–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö) ---
        # –°–±—Ä–æ—Å
        obs_data = ray.get([w.reset.remote() for w in workers])
        histories = [d[0] for d in obs_data]
        trajectories = [[] for _ in range(GROUP_SIZE)]
        active_indices = list(range(GROUP_SIZE))
        finished_rewards = [0] * GROUP_SIZE
        
        step_count = 0
        while active_indices:
            step_count += 1
            # Batch Inference
            active_histories = [histories[i] for i in active_indices]
            prompts = format_batch_prompts(active_histories, tokenizer)
            inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=40, do_sample=True, temperature=0.8, pad_token_id=tokenizer.pad_token_id)
            
            gen_ids = outputs[:, inputs.input_ids.shape[1]:]
            gen_texts = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
            actions = [parse_action(t) for t in gen_texts]
            
            # Env Step
            futures = [workers[active_indices[i]].step.remote(actions[i]) for i in range(len(active_indices))]
            results = ray.get(futures)
            
            next_active = []
            for i, (res_obs, res_reward, res_done, _) in enumerate(results):
                agent_idx = active_indices[i]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
                trajectories[agent_idx].append({
                    "input_ids": inputs.input_ids[i].cpu(),
                    "gen_ids": gen_ids[i].cpu(),
                })
                histories[agent_idx] = res_obs[0]
                
                if res_done:
                    finished_rewards[agent_idx] = res_reward
                else:
                    if step_count < MAX_STEPS:
                        next_active.append(agent_idx)
                    else:
                        finished_rewards[agent_idx] = res_reward # Force stop
            active_indices = next_active

        # --- B. GRPO Update (–û–±—É—á–µ–Ω–∏–µ) ---
        rewards_t = torch.tensor(finished_rewards, dtype=torch.float32, device=model.device)
        mean_r = rewards_t.mean().item()
        std_r = rewards_t.std() + 1e-8
        advantages = (rewards_t - mean_r) / std_r
        
        print(f"  Rewards: {finished_rewards} | Mean: {mean_r:.1f}")
        
        optimizer.zero_grad()
        total_loss = 0
        
        # –û–±—É—á–∞–µ–º –±–∞—Ç—á–∞–º–∏ –ø–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º (–º–æ–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å, –Ω–æ —Ç–∞–∫ –ø—Ä–æ—â–µ –ø–æ –ø–∞–º—è—Ç–∏)
        for i, traj in enumerate(trajectories):
            adv = advantages[i]
            if abs(adv.item()) < 0.1: continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "—Å—Ä–µ–¥–Ω–∏–µ" —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É—á–∏–º—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ö–æ—Ä–æ—à–∏—Ö/–ø–ª–æ—Ö–∏—Ö
            
            for step_data in traj:
                inp = step_data["input_ids"].to(model.device).unsqueeze(0)
                gen = step_data["gen_ids"].to(model.device).unsqueeze(0)
                full = torch.cat([inp, gen], dim=1)
                
                out = model(full)
                logits = out.logits[:, inp.shape[1]-1 : full.shape[1]-1, :]
                
                loss = -F.cross_entropy(logits.transpose(1, 2), gen, reduction='none').sum() * adv
                loss = loss / (len(traj) * GROUP_SIZE)
                loss.backward()
                total_loss += loss.item()
                
        optimizer.step()
        print(f"  Loss: {total_loss:.4f}")
        
        # --- C. Checkpointing (–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ) ---
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if mean_r > best_avg_reward:
            best_avg_reward = mean_r
            save_checkpoint(model, tokenizer, it, mean_r, is_best=True)
            
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ
        if it % SAVE_EVERY == 0:
            save_checkpoint(model, tokenizer, it, mean_r, is_best=False)

except KeyboardInterrupt:
    print("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ...")
    save_checkpoint(model, tokenizer, it, mean_r, is_best=False)

ray.shutdown()
