#!/usr/bin/env python3
"""
–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º:
- Ray –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (10-16x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
- Mixed Precision Training (2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
- torch.compile (1.5-2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ) - –ù–ï –†–ê–ë–û–¢–ê–ï–¢ –ù–ê WINDOWS
- Gradient Accumulation
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataLoader

–û–ë–©–ï–ï –£–°–ö–û–†–ï–ù–ò–ï: 30-100x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–µ–π!
"""

import os
import sys
import argparse
import torch
from datetime import datetime


def run_optimized_pipeline(
    env_preset='cartpole',
    num_episodes=1000,
    num_epochs=50,
    batch_size=64,
    embed_dim=256,
    num_layers=6,
    quick_test=False,
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    use_amp=True,
    use_compile=True,
    num_workers_data=None,
    accumulation_steps=1,
    num_dataloader_workers=4
):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
            use_amp: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Mixed Precision Training
            use_compile: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å torch.compile (PyTorch 2.0+)
            num_workers_data: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ Ray workers –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
            accumulation_steps: —à–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            num_dataloader_workers: workers –¥–ª—è DataLoader
    """
    
    # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    if quick_test:
        print("\n" + "="*80)
        print("–†–ï–ñ–ò–ú –ë–´–°–¢–†–û–ì–û –¢–ï–°–¢–ê")
        print("="*80)
        num_episodes = 100
        num_epochs = 3
        batch_size = 16
        embed_dim = 128
        num_layers = 2
        num_workers_data = 4
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Windows - torch.compile –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    is_windows = sys.platform.startswith('win')
    if is_windows and use_compile:
        print("\n" + "="*80)
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: WINDOWS –û–ë–ù–ê–†–£–ñ–ï–ù–ê")
        print("="*80)
        print("torch.compile —Ç—Ä–µ–±—É–µ—Ç Triton, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ Windows")
        print("torch.compile –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –û–¢–ö–õ–Æ–ß–ï–ù")
        print("–í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç!")
        print("="*80)
        use_compile = False
    
    print("\n" + "="*80)
    print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*80)
    print(f"\nüìä –ü–ê–†–ê–ú–ï–¢–†–´:")
    print(f"  –°—Ä–µ–¥–∞: {env_preset}")
    print(f"  –≠–ø–∏–∑–æ–¥–æ–≤: {num_episodes}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Batch size: {batch_size}")
    print(f"  Accumulation steps: {accumulation_steps}")
    print(f"  Effective batch: {batch_size * accumulation_steps}")
    
    print(f"\nüöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    print(f"  Mixed Precision (AMP): {use_amp and torch.cuda.is_available()}")
    print(f"  torch.compile: {use_compile and hasattr(torch, 'compile')}")
    if is_windows and not use_compile:
        print(f"    ‚ö†Ô∏è  torch.compile –æ—Ç–∫–ª—é—á–µ–Ω (Windows)")
    print(f"  Ray workers: {num_workers_data if num_workers_data else 'auto'}")
    print(f"  DataLoader workers: {num_dataloader_workers}")
    
    # ========================================================================
    # –≠–¢–ê–ü 1: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–• –° RAY
    # ========================================================================
    print("\n" + "="*80)
    print("–≠–¢–ê–ü 1: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–• (RAY)")
    print("="*80)
    
    import ray
    from gym_data_parallel import parallel_collect_data
    
    data_path = f'data/{env_preset}_parallel.pkl'
    
    if not os.path.exists(data_path):
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä {num_episodes} —ç–ø–∏–∑–æ–¥–æ–≤...")
        
        trajectories, env_config = parallel_collect_data(
            env_preset=env_preset,
            num_episodes=num_episodes,
            max_episode_length=500,
            num_workers=num_workers_data,
            save_path=data_path,
            seed=42,
            verbose=True
        )
        
        print(f"\n‚úì –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω!")
    else:
        print(f"\n‚úì –î–∞–Ω–Ω—ã–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {data_path}")
        import pickle
        with open(data_path, 'rb') as f:
            data = pickle.load(f)
        trajectories = data['trajectories']
        env_config = data['env_config']
        
        print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π: {len(trajectories)}")
        if 'collection_time' in data:
            print(f"  –í—Ä–µ–º—è —Å–±–æ—Ä–∞: {data['collection_time']:.2f}s")
        if 'num_workers' in data:
            print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ workers: {data['num_workers']}")
    
    # –í—ã–∫–ª—é—á–∞–µ–º Ray –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    if ray.is_initialized():
        ray.shutdown()
    
    # ========================================================================
    # –≠–¢–ê–ü 2: –ü–û–î–ì–û–¢–û–í–ö–ê DATALOADER –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø–ú–ò
    # ========================================================================
    print("\n" + "="*80)
    print("–≠–¢–ê–ü 2: –°–û–ó–î–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• DATALOADERS")
    print("="*80)
    
    from gym_data_preparation import create_dataloaders
    
    obs_dim = env_config['obs_dim']
    action_dim = env_config['action_dim']
    
    print(f"\n–°–æ–∑–¥–∞–µ–º DataLoaders —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏...")
    train_loader, val_loader = create_dataloaders(
        trajectories,
        context_length=20,
        action_dim=action_dim,
        batch_size=batch_size,
        train_split=0.9,
        num_workers=num_dataloader_workers,
        pin_memory=True,
        persistent_workers=True if num_dataloader_workers > 0 else None,
        prefetch_factor=2 if num_dataloader_workers > 0 else None
    )
    
    print(f"‚úì DataLoaders –≥–æ—Ç–æ–≤—ã —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏:")
    if num_dataloader_workers > 0:
        print(f"  pin_memory: True")
        print(f"  persistent_workers: True")
        print(f"  prefetch_factor: 2")
    
    # ========================================================================
    # –≠–¢–ê–ü 3: –°–û–ó–î–ê–ù–ò–ï –ò –ö–û–ú–ü–ò–õ–Ø–¶–ò–Ø –ú–û–î–ï–õ–ò
    # ========================================================================
    print("\n" + "="*80)
    print("–≠–¢–ê–ü 3: –°–û–ó–î–ê–ù–ò–ï –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò")
    print("="*80)
    
    from model import DecisionTransformer
    
    config = {
        'obs_dim': obs_dim,
        'action_dim': action_dim,
        'context_length': 20,
        
        'embed_dim': embed_dim,
        'num_layers': num_layers,
        'num_heads': 8,
        'num_kv_heads': 4,
        'num_experts': 8,
        'max_seq_len': 512,
        'dropout': 0.1,
        
        'batch_size': batch_size,
        'num_epochs': num_epochs,
        'learning_rate': 3e-4,
        'weight_decay': 0.01,
        'grad_clip': 1.0,
        'train_split': 0.9,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        'use_amp': use_amp and torch.cuda.is_available(),
        'use_compile': use_compile and hasattr(torch, 'compile'),
        'accumulation_steps': accumulation_steps,
        
        'log_interval': 10,
        'save_interval': max(1, num_epochs // 5),
        'checkpoint_dir': f'checkpoints/gym_{env_preset}_optimized_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'log_dir': f'logs/gym_{env_preset}_optimized_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        
        'num_workers': num_dataloader_workers,
        'seed': 42
    }
    
    torch.manual_seed(config['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config['seed'])
    
    print(f"\n–°–æ–∑–¥–∞–µ–º Decision Transformer...")
    model = DecisionTransformer(
        obs_dim=config['obs_dim'],
        action_dim=config['action_dim'],
        embed_dim=config['embed_dim'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        num_kv_heads=config['num_kv_heads'],
        num_experts=config['num_experts'],
        max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    # ========================================================================
    # –≠–¢–ê–ü 4: –û–ë–£–ß–ï–ù–ò–ï –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø–ú–ò
    # ========================================================================
    print("\n" + "="*80)
    print("–≠–¢–ê–ü 4: –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï")
    print("="*80)
    
    from train_optimized import OptimizedTrainer
    
    trainer = OptimizedTrainer(model, train_loader, val_loader, config)
    
    print(f"\nüéØ –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ:")
    speedup = 1.0
    if config['use_amp']:
        print(f"  Mixed Precision: ~2-3x")
        speedup *= 2.5
    if config['use_compile']:
        print(f"  torch.compile: ~1.5-2x")
        speedup *= 1.75
    if accumulation_steps > 1:
        print(f"  Gradient Accumulation: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch {batch_size * accumulation_steps}")
    print(f"\n  –û–ë–©–ï–ï –£–°–ö–û–†–ï–ù–ò–ï –û–ë–£–ß–ï–ù–ò–Ø: ~{speedup:.1f}x")
    
    print(f"\n{'='*80}")
    print(f"–ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï...")
    print(f"{'='*80}\n")
    
    trainer.train()
    
    # ========================================================================
    # –≠–¢–ê–ü 5: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
    # ========================================================================
    print("\n" + "="*80)
    print("–≠–¢–ê–ü 5: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*80)
    
    from inference import AgentInference
    from gym_environment import create_gym_environment
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    checkpoint_path = os.path.join(config['checkpoint_dir'], 'best_model.pt')
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞, –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    test_model = DecisionTransformer(
        obs_dim=config['obs_dim'],
        action_dim=config['action_dim'],
        embed_dim=config['embed_dim'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        num_kv_heads=config['num_kv_heads'],
        num_experts=config['num_experts'],
        max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    )
    test_model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (val loss: {checkpoint['best_val_loss']:.4f})")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    agent = AgentInference(test_model)
    env = create_gym_environment(env_preset, seed=100)
    
    num_test_episodes = 5 if not quick_test else 2
    print(f"\n–ó–∞–ø—É—Å–∫–∞–µ–º {num_test_episodes} —Ç–µ—Å—Ç–æ–≤—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤...")
    
    total_rewards = []
    episode_lengths = []
    
    for episode in range(num_test_episodes):
        obs = env.reset()
        agent.reset()
        
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.select_action(obs, temperature=1.0)
            obs, reward, done, _ = env.step(action)
            
            episode_reward += reward
            steps += 1
            
            if done:
                break
        
        total_rewards.append(episode_reward)
        episode_lengths.append(steps)
        
        print(f"  –≠–ø–∏–∑–æ–¥ {episode + 1}: {steps} —à–∞–≥–æ–≤, reward={episode_reward:.2f}")
    
    avg_reward = sum(total_rewards) / len(total_rewards)
    avg_length = sum(episode_lengths) / len(episode_lengths)
    
    print(f"\n‚úì –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.2f}")
    print(f"‚úì –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.1f}")
    
    env.close()
    
    # ========================================================================
    # –ò–¢–û–ì–ò
    # ========================================================================
    print("\n" + "="*80)
    print("üéâ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù!")
    print("="*80)
    
    print(f"\nüìÅ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"  –ú–æ–¥–µ–ª—å: {config['checkpoint_dir']}")
    print(f"  –õ–æ–≥–∏: {config['log_dir']}")
    
    print(f"\nüìä –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    optimizations = []
    if config['use_amp']:
        optimizations.append("‚úì Mixed Precision (AMP)")
    if config['use_compile']:
        optimizations.append("‚úì torch.compile")
    if accumulation_steps > 1:
        optimizations.append(f"‚úì Gradient Accumulation (x{accumulation_steps})")
    optimizations.append("‚úì –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (Ray)")
    optimizations.append("‚úì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataLoader")
    
    for opt in optimizations:
        print(f"  {opt}")
    
    print(f"\nüí° –î–õ–Ø –ü–†–û–°–ú–û–¢–†–ê –õ–û–ì–û–í:")
    print(f"  tensorboard --logdir {config['log_dir']}")
    
    print(f"\n{'='*80}\n")
    
    return config['checkpoint_dir']


def main():
    parser = argparse.ArgumentParser(
        description='–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Gymnasium'
    )
    
    parser.add_argument('--env', type=str, default='cartpole',
                        choices=['cartpole', 'lunar_lander', 'mountain_car', 'acrobot', 'pendulum'],
                        help='Preset —Å—Ä–µ–¥—ã')
    parser.add_argument('--num_episodes', type=int, default=1000,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞')
    parser.add_argument('--num_epochs', type=int, default=50,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--embed_dim', type=int, default=256,
                        help='–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings')
    parser.add_argument('--num_layers', type=int, default=6,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤')
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    parser.add_argument('--use_amp', action='store_true', default=True,
                        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Mixed Precision Training (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)')
    parser.add_argument('--no_amp', action='store_false', dest='use_amp',
                        help='–û—Ç–∫–ª—é—á–∏—Ç—å Mixed Precision')
    parser.add_argument('--use_compile', action='store_true', default=True,
                        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å torch.compile (PyTorch 2.0+)')
    parser.add_argument('--no_compile', action='store_false', dest='use_compile',
                        help='–û—Ç–∫–ª—é—á–∏—Ç—å torch.compile')
    parser.add_argument('--num_workers_data', type=int, default=None,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Ray workers –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (auto –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)')
    parser.add_argument('--accumulation_steps', type=int, default=1,
                        help='–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤')
    parser.add_argument('--num_dataloader_workers', type=int, default=4,
                        help='Workers –¥–ª—è DataLoader')
    
    parser.add_argument('--quick_test', action='store_true',
                        help='–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*80)
    print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for arg, value in vars(args).items():
        print(f"  {arg}: {value}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    if torch.cuda.is_available():
        print(f"\n‚úì CUDA –¥–æ—Å—Ç—É–ø–Ω–∞")
        print(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
        print(f"  –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print(f"\n‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU")
        print(f"  Mixed Precision –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω")
    
    try:
        checkpoint_dir = run_optimized_pipeline(
            env_preset=args.env,
            num_episodes=args.num_episodes,
            num_epochs=args.num_epochs,
            batch_size=args.batch_size,
            embed_dim=args.embed_dim,
            num_layers=args.num_layers,
            quick_test=args.quick_test,
            use_amp=args.use_amp,
            use_compile=args.use_compile,
            num_workers_data=args.num_workers_data,
            accumulation_steps=args.accumulation_steps,
            num_dataloader_workers=args.num_dataloader_workers
        )
        
        print("\n‚úì –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        sys.exit(0)
        
    except KeyboardInterrupt:
        print("\n\n‚úó –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚úó –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
