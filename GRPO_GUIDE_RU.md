# üéØ GRPO –û–±—É—á–µ–Ω–∏–µ - –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø (Reinforcement Learning)

## –ß—Ç–æ —Ç–∞–∫–æ–µ GRPO?

**GRPO (Group Relative Policy Optimization)** - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è PPO (Proximal Policy Optimization) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.

### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç PPO:

1. **–ì—Ä—É–ø–ø–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è advantage** - –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, advantage –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø
2. **–ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
3. **–õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏** - –æ—Å–æ–±–µ–Ω–Ω–æ —Å –±–æ–ª—å—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

---

## üìã –î–≤–∞ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è

### –≠—Ç–∞–ø 1: Supervised Learning (Behavior Cloning)
```bash
# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π
python run_gym_pipeline_optimized.py --env cartpole
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –±–∞–∑–æ–≤–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.

### –≠—Ç–∞–ø 2: GRPO (Reinforcement Learning)  
```bash
# –£–ª—É—á—à–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π
python run_grpo.py \
  --checkpoint checkpoints/gym_cartpole_*/best_model.pt \
  --env cartpole
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É, –ø–æ–ª—É—á–∞—è —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∏–∑ —Å—Ä–µ–¥—ã.

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –®–∞–≥ 1: Supervised Learning (—ç—Ç–∞–ø 1)

```bash
# –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
python run_gym_pipeline_optimized.py \
  --env cartpole \
  --num_episodes 1000 \
  --num_epochs 50
```

**–í—Ä–µ–º—è:** ~10-15 –º–∏–Ω—É—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ `checkpoints/gym_cartpole_*/best_model.pt`

---

### –®–∞–≥ 2: GRPO –û–±—É—á–µ–Ω–∏–µ (—ç—Ç–∞–ø 2)

```bash
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –æ–±—É—á–∞–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
python run_grpo.py \
  --checkpoint checkpoints/gym_cartpole_20241108_123456/best_model.pt \
  --env cartpole \
  --num_iterations 100
```

**–í—Ä–µ–º—è:** ~20-30 –º–∏–Ω—É—Ç

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –≤ `checkpoints/grpo_cartpole_*/`

---

### –®–∞–≥ 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
python test_grpo_agent.py \
  --checkpoint checkpoints/grpo_cartpole_*/iteration_100.pt \
  --env cartpole \
  --num_episodes 10 \
  --deterministic
```

---

## üìä –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç GRPO

### –ê–ª–≥–æ—Ä–∏—Ç–º:

```
1. Collect Rollouts (—Å–æ–±–∏—Ä–∞–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏):
   - –ó–∞–ø—É—Å–∫–∞–µ–º N —Å—Ä–µ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
   - –°–æ–±–∏—Ä–∞–µ–º T —à–∞–≥–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–π —Å—Ä–µ–¥—ã
   - –°–æ—Ö—Ä–∞–Ω—è–µ–º: obs, actions, rewards, values, log_probs

2. Compute Advantages (–≤—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞):
   - –ò—Å–ø–æ–ª—å–∑—É–µ–º GAE (Generalized Advantage Estimation)
   - –ü—Ä–∏–º–µ–Ω—è–µ–º GROUP NORMALIZATION (–∫–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ GRPO)
   - –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º advantage –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø, –∞ –Ω–µ –≥–ª–æ–±–∞–ª—å–Ω–æ

3. Update Policy (–æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É):
   - –ù–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - PPO clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
   - –û–±–Ω–æ–≤–ª—è–µ–º policy –∏ value network

4. Repeat (–ø–æ–≤—Ç–æ—Ä—è–µ–º)
```

### –ì—Ä—É–ø–ø–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (GRPO):

```python
# –û–±—ã—á–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (PPO):
advantages = (advantages - mean(advantages)) / std(advantages)

# –ì—Ä—É–ø–ø–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (GRPO):
for group in split_into_groups(advantages, group_size=8):
    group = (group - mean(group)) / std(group)
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ª—É—á—à–µ?**
- –ú–µ–Ω—å—à–µ –≤–ª–∏—è–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
- –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
- –õ—É—á—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π

---

## üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã GRPO

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

```python
# RL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
gamma = 0.99            # Discount factor
gae_lambda = 0.95       # GAE –ø–∞—Ä–∞–º–µ—Ç—Ä
clip_epsilon = 0.2      # PPO clipping
entropy_coef = 0.01     # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏
value_coef = 0.5        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç value loss

# GRPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
group_size = 8          # –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

# –°—Ä–µ–¥–∞
num_envs = 8            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥
rollout_steps = 128     # –®–∞–≥–æ–≤ –≤ –∫–∞–∂–¥–æ–º rollout

# –û–±—É—á–µ–Ω–∏–µ
num_epochs = 4          # –≠–ø–æ—Ö –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
batch_size = 64         # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
policy_lr = 3e-5        # Learning rate (–º–µ–Ω—å—à–µ —á–µ–º –Ω–∞ SL!)
value_lr = 1e-4         # LR –¥–ª—è value network
```

### –ö–∞–∫ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

**–ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ:**
- –£–º–µ–Ω—å—à–∏—Ç–µ `policy_lr` (3e-5 ‚Üí 1e-5)
- –£–≤–µ–ª–∏—á—å—Ç–µ `clip_epsilon` (0.2 ‚Üí 0.3)
- –£–º–µ–Ω—å—à–∏—Ç–µ `entropy_coef` (0.01 ‚Üí 0.001)

**–ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ–µ:**
- –£–≤–µ–ª–∏—á—å—Ç–µ `num_envs` (8 ‚Üí 16)
- –£–≤–µ–ª–∏—á—å—Ç–µ `rollout_steps` (128 ‚Üí 256)
- –£–≤–µ–ª–∏—á—å—Ç–µ `policy_lr` (3e-5 ‚Üí 1e-4)

**–ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç exploration:**
- –£–≤–µ–ª–∏—á—å—Ç–µ `entropy_coef` (0.01 ‚Üí 0.05)
- –£–º–µ–Ω—å—à–∏—Ç–µ `clip_epsilon` (0.2 ‚Üí 0.1)

---

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

### TensorBoard

```bash
tensorboard --logdir logs/grpo_cartpole_*
```

**–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:**

1. **mean_reward** - —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥
   - –î–æ–ª–∂–Ω–∞ —Ä–∞—Å—Ç–∏ ‚ÜóÔ∏è
   - –ï—Å–ª–∏ —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç - –ø—Ä–æ–±–ª–µ–º—ã —Å exploration

2. **policy_loss** - loss –ø–æ–ª–∏—Ç–∏–∫–∏
   - –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º
   - –ï—Å–ª–∏ —Å–∫–∞—á–µ—Ç - —É–º–µ–Ω—å—à–∏—Ç–µ LR

3. **value_loss** - loss value network
   - –î–æ–ª–∂–µ–Ω —É–º–µ–Ω—å—à–∞—Ç—å—Å—è
   - –ï—Å–ª–∏ –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è - –ø—Ä–æ–±–ª–µ–º—ã —Å critic

4. **entropy** - —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
   - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç exploration
   - –ï—Å–ª–∏ –ø–∞–¥–∞–µ—Ç –¥–æ –Ω—É–ª—è - policy —Å–ª–∏—à–∫–æ–º deterministic

---

## üéÆ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç

```bash
# –≠—Ç–∞–ø 1: Supervised Learning (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)
python run_gym_pipeline_optimized.py --quick_test

# –≠—Ç–∞–ø 2: GRPO (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)
python run_grpo.py \
  --checkpoint checkpoints/.../best_model.pt \
  --quick_test

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python test_grpo_agent.py \
  --checkpoint checkpoints/grpo_*/iteration_10.pt \
  --num_episodes 5
```

**–í—Ä–µ–º—è:** ~5 –º–∏–Ω—É—Ç –Ω–∞ –≤–µ—Å—å —Ü–∏–∫–ª

---

### –ü—Ä–∏–º–µ—Ä 2: CartPole (–ø—Ä–æ—Å—Ç–∞—è —Å—Ä–µ–¥–∞)

```bash
# –≠—Ç–∞–ø 1
python run_gym_pipeline_optimized.py \
  --env cartpole \
  --num_episodes 1000 \
  --num_epochs 50

# –≠—Ç–∞–ø 2
python run_grpo.py \
  --checkpoint checkpoints/.../best_model.pt \
  --env cartpole \
  --num_iterations 100 \
  --num_envs 8

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python test_grpo_agent.py \
  --checkpoint checkpoints/.../iteration_100.pt \
  --env cartpole \
  --deterministic
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ù–∞–≥—Ä–∞–¥–∞ 500 (–º–∞–∫—Å–∏–º—É–º –¥–ª—è CartPole)

---

### –ü—Ä–∏–º–µ—Ä 3: LunarLander (—Å–ª–æ–∂–Ω–∞—è —Å—Ä–µ–¥–∞)

```bash
# –≠—Ç–∞–ø 1
python run_gym_pipeline_optimized.py \
  --env lunar_lander \
  --num_episodes 3000 \
  --num_epochs 100

# –≠—Ç–∞–ø 2
python run_grpo.py \
  --checkpoint checkpoints/.../best_model.pt \
  --env lunar_lander \
  --num_iterations 500 \
  --num_envs 16 \
  --rollout_steps 256

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python test_grpo_agent.py \
  --checkpoint checkpoints/.../iteration_500.pt \
  --env lunar_lander \
  --num_episodes 20
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ù–∞–≥—Ä–∞–¥–∞ > 200 (—Ö–æ—Ä–æ—à–∞—è –ø–æ—Å–∞–¥–∫–∞)

---

### –ü—Ä–∏–º–µ—Ä 4: –°–≤–æ—è —Å—Ä–µ–¥–∞

```python
# 1. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Å—Ä–µ–¥—É (–∫–∞–∫ –æ–±—ã—á–Ω–æ)
import gymnasium as gym

gym.register(id='MyEnv-v0', entry_point='my_module:MyEnv')

# 2. Supervised Learning
python run_gym_pipeline_optimized.py --env MyEnv-v0

# 3. GRPO (—É–∫–∞–∂–∏—Ç–µ —Å–≤–æ—é —Å—Ä–µ–¥—É —á–µ—Ä–µ–∑ –∫–æ–¥)
# –í run_grpo.py –∏–∑–º–µ–Ω–∏—Ç–µ env_fn:

def env_fn():
    return create_gym_environment('MyEnv-v0')
```

---

## üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

### –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π clip_epsilon

```python
# –í train_grpo.py –¥–æ–±–∞–≤—å—Ç–µ:
def get_clip_epsilon(iteration, max_iterations):
    # –£–º–µ–Ω—å—à–∞–µ–º clipping —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
    initial_clip = 0.2
    final_clip = 0.05
    return initial_clip - (initial_clip - final_clip) * (iteration / max_iterations)
```

### –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—ç–º–ø–ª–∏–Ω–≥

```python
# –°—ç–º–ø–ª–∏—Ä—É–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤—ã—Å–æ–∫–∏–º advantage —á–∞—â–µ
sample_weights = np.abs(advantages.numpy())
sample_weights = sample_weights / sample_weights.sum()
```

### Curriculum Learning

```python
# –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è–µ–º —Å—Ä–µ–¥—É
def create_curriculum_env(difficulty):
    if difficulty < 0.3:
        return EasyEnv()
    elif difficulty < 0.7:
        return MediumEnv()
    else:
        return HardEnv()
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | Sample Efficiency |
|-------|--------------|----------|-------------------|
| **GRPO** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| PPO | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| SAC | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| DQN | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê |

**GRPO –ª—É—á—à–µ –∫–æ–≥–¥–∞:**
- –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã)
- –ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- –ï—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å—Ä–µ–¥—ã

---

## üêõ Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Mean reward –Ω–µ —Ä–∞—Å—Ç–µ—Ç

**–ü—Ä–∏—á–∏–Ω—ã:**
1. Policy –∑–∞—Å—Ç—Ä—è–ª–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∏–Ω–∏–º—É–º–µ
2. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ exploration
3. –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π learning rate

**–†–µ—à–µ–Ω–∏—è:**
```bash
# –£–≤–µ–ª–∏—á—å—Ç–µ entropy
--entropy_coef 0.05

# –£–≤–µ–ª–∏—á—å—Ç–µ LR
--policy_lr 1e-4

# –°–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
--num_envs 16 --rollout_steps 256
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: Policy loss –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π / NaN

**–ü—Ä–∏—á–∏–Ω—ã:**
1. –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π learning rate
2. Gradient exploding
3. –ü—Ä–æ–±–ª–µ–º—ã —Å advantage –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π

**–†–µ—à–µ–Ω–∏—è:**
```bash
# –£–º–µ–Ω—å—à–∏—Ç–µ LR
--policy_lr 1e-5

# –£–≤–µ–ª–∏—á—å—Ç–µ gradient clipping
max_grad_norm = 0.3  # –≤ –∫–æ–¥–µ

# –£–≤–µ–ª–∏—á—å—Ç–µ group size
group_size = 16  # –≤ –∫–æ–¥–µ
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: Value loss –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è

**–ü—Ä–∏—á–∏–Ω—ã:**
1. Value network —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è
2. LR –¥–ª—è value —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π
3. –ù–∞–≥—Ä–∞–¥—ã –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã

**–†–µ—à–µ–Ω–∏—è:**
```python
# –£–≤–µ–ª–∏—á—å—Ç–µ value network
value_net = ValueNetwork(
    obs_dim=obs_dim,
    embed_dim=512,  # –ë—ã–ª–æ 256
    hidden_dim=1024  # –ë—ã–ª–æ 512
)

# –£–≤–µ–ª–∏—á—å—Ç–µ value LR
value_lr = 3e-4  # –ë—ã–ª–æ 1e-4
```

---

## üí° –°–æ–≤–µ—Ç—ã –∏ best practices

### 1. –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å supervised learning

Supervised learning –¥–∞–µ—Ç —Ö–æ—Ä–æ—à—É—é –Ω–∞—á–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É.

### 2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å—Ä–µ–¥—ã

–ë–æ–ª—å—à–µ —Å—Ä–µ–¥ = –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö = –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ.

### 3. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ entropy

–ï—Å–ª–∏ entropy –ø–∞–¥–∞–µ—Ç –¥–æ –Ω—É–ª—è - —É–≤–µ–ª–∏—á—å—Ç–µ `entropy_coef`.

### 4. –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã —á–∞—Å—Ç–æ

GRPO –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º, —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∫–∞–∂–¥—ã–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π.

### 5. –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ

–ó–∞–ø—É—Å–∫–∞–π—Ç–µ —Ç–µ—Å—Ç—ã –∫–∞–∂–¥—ã–µ 50-100 –∏—Ç–µ—Ä–∞—Ü–∏–π —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å.

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –°—Ç–∞—Ç—å–∏:
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Decision Transformer](https://arxiv.org/abs/2106.01345)

### –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
- PPO (Proximal Policy Optimization)
- TRPO (Trust Region Policy Optimization)
- SAC (Soft Actor-Critic)

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç GRPO –æ–±—É—á–µ–Ω–∏—è

- [ ] –û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å –Ω–∞ supervised learning (—ç—Ç–∞–ø 1)
- [ ] –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç —Å —ç—Ç–∞–ø–∞ 1
- [ ] –°–æ–∑–¥–∞–Ω–∞ value network
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã GRPO
- [ ] –ó–∞–ø—É—â–µ–Ω–æ GRPO –æ–±—É—á–µ–Ω–∏–µ
- [ ] Mean reward —Ä–∞—Å—Ç–µ—Ç
- [ ] Losses —Å—Ç–∞–±–∏–ª—å–Ω—ã
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –∞–≥–µ–Ω—Ç
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ

---

**–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∞—Ç—å Decision Transformer —Å GRPO!** üéâ

–î–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:

```bash
# –≠—Ç–∞–ø 1
python run_gym_pipeline_optimized.py --env cartpole

# –≠—Ç–∞–ø 2  
python run_grpo.py --checkpoint checkpoints/.../best_model.pt
```
